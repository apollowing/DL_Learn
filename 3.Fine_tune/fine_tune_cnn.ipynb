{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning model to recognize opened or closed window\n",
    "Use cases: check if windows are closed if raining or when leaving house\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "# libraries for displaying images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from skimage import exposure\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform all installations\n",
    "# !pip install tensorflow-gpu==2.5.0\n",
    "# !pip install tensorflow-datasets\n",
    "# !pip install tensorwatch\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# tfds makes a lot of progress bars and they take up a lot of screen space, so lets diable them\n",
    "# tfds.disable_progress_bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "NUM_CLASSES = 2\n",
    "TRAIN_DATA_DIR = r\"../contents/windows_imgs/train\"\n",
    "VAL_DATA_DIR = r\"../contents/windows_imgs/val\"\n",
    "DATA_DIR = r\"../contents/windows_imgs/data\"\n",
    "TEST_DATA_DIR = r\"../contents/windows_imgs/test\"\n",
    "LOG_DIR = './log'\n",
    "TRAIN_SAMPLES = 9\n",
    "VAL_SAMPLES = 4\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "CLASS_THRESHOLD = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "from datetime import datetime\n",
    "ts = datetime.now()\n",
    "ts = f\"{ts:%Y%m%d-%H%M%S}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "load image data from directory and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import left_shift, right_shift\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode = 'nearest',\n",
    "    brightness_range=(0.8,1.2),\n",
    "    # shear_range=0.2,\n",
    "    # featurewise_center =True,\n",
    "    # horizontal_flip=True,\n",
    "    # validation_split=0.1\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 images belonging to 2 classes.\n",
      "Found 32 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(WIDTH, HEIGHT),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    seed = 12345,\n",
    "    class_mode = 'binary',\n",
    "    # subset='training',\n",
    "    save_to_dir=r\"../contents/windows_imgs/output/train/\",\n",
    "    save_prefix=\"\",\n",
    "    save_format='jpg')\n",
    "TRAIN_SAMPLES = train_generator.samples\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DATA_DIR,\n",
    "    target_size=(WIDTH, HEIGHT),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    class_mode = 'binary',\n",
    "    # subset='validation',\n",
    "    save_to_dir=r\"../contents/windows_imgs/output/val/\",\n",
    "    save_prefix=\"\",\n",
    "    save_format='jpg')\n",
    "VAL_SAMPLES = val_generator.samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to show the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the figure plot area\n",
    "# # fig, ax = plt.subplots(1,NUM_IMG, figsize=(12,4))\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for i in range(BATCH_SIZE):\n",
    "#     print(f'Run #{i}')\n",
    "#     img, label = train_generator.next()\n",
    "#     # classval = np.argmax(label[0], axis = 0)    # convert one-hot encoding to index\n",
    "#     # classkey = find_key(classval, iter_img.class_indices) # get class key from index value\n",
    "#     plt.imshow(img[0])\n",
    "#     plt.show()\n",
    "#     # print(f'img shape {img.shape}')\n",
    "#     # print(f'img label {classkey}\\n')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# # cv2.waitKey(0)\n",
    "# cam = cv2.VideoCapture(0)\n",
    "\n",
    "# result, cam_img = cam.read()\n",
    "# if result:\n",
    "#     # cv2.imshow(\"Captured image\", cam_img)\n",
    "#     cv2.imwrite(TEST_DATA_DIR + r\"\\test_window.jpeg\", cam_img)\n",
    "#     # cv2.waitKey(0)\n",
    "#     # cv2.destroyAllWindows()\n",
    "# else:\n",
    "#     print(\"No image detected. Please try again.\") \n",
    "\n",
    "\n",
    "# from sympy import plot\n",
    "\n",
    "\n",
    "# img_path = r'../contents/windows_imgs/test/test_window.jpeg'\n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "# img_array = image.img_to_array(img)\n",
    "# expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "# preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "# prediction = model.predict(preprocessed_img)\n",
    "# print(prediction)\n",
    "# print(val_generator.class_indices)\n",
    "\n",
    "# # plt.figure(1,1)\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pre-trained modfel and add in fine tuning layers\n",
    "Now we define the model layers to freeze, discard to classification layers\n",
    "then finally add in the fine tuning layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_maker():\n",
    "    # load model\n",
    "    base_model = MobileNet(\n",
    "        include_top=False,\n",
    "        input_shape=(WIDTH, HEIGHT,3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    input = Input(shape=(WIDTH, HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    # custom_model = Flatten()(custom_model)\n",
    "    custom_model = Dense(256, activation = 'sigmoid')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(1, activation='sigmoid')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_maker():\n",
    "    custom_model = Sequential()\n",
    "    # load model\n",
    "    base_model = MobileNet(\n",
    "        include_top=False,\n",
    "        input_shape=(WIDTH, HEIGHT,3))\n",
    "\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    custom_model.add(base_model)\n",
    "    custom_model.add(GlobalAveragePooling2D())\n",
    "    # custom_model.add(Flatten())\n",
    "\n",
    "    # for num in range(num_layers):\n",
    "    custom_model.add(Dense(640, activation = 'relu'))\n",
    "    custom_model.add(Dropout(0.3))\n",
    "\n",
    "    custom_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    return custom_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_maker_MobileNet():\n",
    "    custom_model = Sequential()\n",
    "    # load model\n",
    "    base_model = MobileNet(\n",
    "        include_top=False,\n",
    "        input_shape=(WIDTH, HEIGHT,3))\n",
    "\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    custom_model.add(base_model)\n",
    "    custom_model.add(GlobalAveragePooling2D())\n",
    "    custom_model.add(Flatten())\n",
    "    custom_model.add(Dense(128, activation = 'relu'))\n",
    "    custom_model.add(Dropout(0.3))\n",
    "\n",
    "    # use sigmoid for binary classification\n",
    "    custom_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    custom_model.compile(\n",
    "        loss =  'binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just trying out with another pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "\n",
    "def model_maker_inceptionResNetV2():\n",
    "    # load model\n",
    "    base_model = InceptionResNetV2(\n",
    "        include_top=False,\n",
    "        input_shape=(WIDTH, HEIGHT,3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    input = Input(shape=(WIDTH, HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Flatten()(custom_model)\n",
    "    custom_model = Dense(256, activation = 'relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(1, activation='sigmoid')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with another model from scratch, however it perform poorly. I am not sure why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "# from tensorflow.keras.optimizers import SGD,RMSprop,adam\n",
    "\n",
    "def model_maker_scratch():\n",
    "    from_scratch = Sequential()\n",
    "    from_scratch.add(Convolution2D(224, 3,3,input_shape=(WIDTH, HEIGHT, 3)))\n",
    "    from_scratch.add(Activation('relu'))\n",
    "    from_scratch.add(Convolution2D(1000, 3, 3))\n",
    "    from_scratch.add(Activation('relu'))\n",
    "    from_scratch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    from_scratch.add(Dropout(0.5))\n",
    "    # from_scratch.add(Convolution2D(64, 3, 3))\n",
    "    from_scratch.add(Convolution2D(1000, 3, 3))\n",
    "    from_scratch.add(Activation('relu'))\n",
    "    from_scratch.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    from_scratch.add(Dropout(0.5))\n",
    "    # \n",
    "\n",
    "    from_scratch.add(Dense(256, activation='relu'))\n",
    "    from_scratch.add(Dropout(0.5))\n",
    "    from_scratch.add(Dense(1, activation='sigmoid'))\n",
    "    # from_scratch.add(Dense(1))\n",
    "    # from_scratch.add(Activation('sigmoid'))\n",
    "    return(from_scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functi  (None, 7, 7, 1024)        3228864   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1024)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 640)               656000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 640)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 641       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3885505 (14.82 MB)\n",
      "Trainable params: 2812993 (10.73 MB)\n",
      "Non-trainable params: 1072512 (4.09 MB)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# switch between models\n",
    "# model = model_maker_scratch()\n",
    "# model = model_maker_inceptionResNetV2()\n",
    "# model = model_maker()\n",
    "\n",
    "# model.compile(\n",
    "#     loss = 'binary_crossentropy',\n",
    "#     optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "#     metrics=['accuracy'])\n",
    "\n",
    "model = model_maker_MobileNet()\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta=0.001, patience=20)\n",
    "\n",
    "#Allow TensorBoard callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"{LOG_DIR}\\{ts}\",\n",
    "                                                      histogram_freq=1,\n",
    "                                                      write_graph=True,\n",
    "                                                      write_grads=True,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      write_images=True)\n",
    "\n",
    "csv_logger = CSVLogger('windows-training-' + 'log.csv',\n",
    "                        append=True,\n",
    "                        separator=';')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6979 - accuracy: 0.6176 - val_loss: 2.2020 - val_accuracy: 0.5000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.9519 - accuracy: 0.4412 - val_loss: 0.8176 - val_accuracy: 0.3750\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.3226 - accuracy: 0.8824 - val_loss: 1.0602 - val_accuracy: 0.5625\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.7427 - accuracy: 0.6765 - val_loss: 0.8378 - val_accuracy: 0.4688\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1652 - accuracy: 0.9118 - val_loss: 1.2038 - val_accuracy: 0.5625\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0995 - accuracy: 0.9706 - val_loss: 1.8649 - val_accuracy: 0.5000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1245 - accuracy: 0.9412 - val_loss: 2.1229 - val_accuracy: 0.5312\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1759 - accuracy: 0.9118 - val_loss: 1.9034 - val_accuracy: 0.5000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 1.6781 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.5143 - val_accuracy: 0.4688\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0516 - accuracy: 0.9706 - val_loss: 1.4904 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0492 - accuracy: 0.9706 - val_loss: 1.5257 - val_accuracy: 0.5000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.5891 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.6706 - val_accuracy: 0.4375\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.7453 - val_accuracy: 0.4375\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.8121 - val_accuracy: 0.4688\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.8718 - val_accuracy: 0.4688\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.9244 - val_accuracy: 0.4688\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.9569 - val_accuracy: 0.4688\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.9840 - val_accuracy: 0.4375\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 7.4137e-04 - accuracy: 1.0000 - val_loss: 2.0168 - val_accuracy: 0.4375\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.0493 - val_accuracy: 0.4375\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.0977 - val_accuracy: 0.4375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gc import callbacks\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES)/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[tensorboard_callback,csv_logger,early_stopping],\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=math.ceil(float(VAL_SAMPLES)/BATCH_SIZE))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# Start TensorBoard\n",
    "%tensorboard --logdir ./log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a 'history' object with training and validation loss\n",
    "# Example: history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "# Retrieve training and validation loss\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss', marker='x')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = f'..\\contents\\models\\winclass_{ts}.h5'\n",
    "model.save(h5_path)\n",
    "tf.saved_model.save(model, \"tmp/model/1/\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Specify the file you want to copy\n",
    "source_file = h5_path\n",
    "destination_file = 'model.h5'\n",
    "\n",
    "# Copy the file\n",
    "shutil.copyfile(source_file, destination_file)\n",
    "\n",
    "print(f\"File copied successfully from {source_file} to {destination_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "result, cam_img = cam.read()\n",
    "if result:\n",
    "    \n",
    "    # cv2.imshow(\"Captured image\", cam_img)\n",
    "    # equ_img = cv2.equalizeHist(cam_img)\n",
    "    cv2.imwrite(TEST_DATA_DIR + r\"\\test_window.jpeg\", cam_img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"No image detected. Please try again.\") \n",
    "\n",
    "from skimage import exposure, transform\n",
    "from skimage.io import imread\n",
    "\n",
    "img_path = r'../contents/windows_imgs/test/test_window.jpeg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# img = imread(img_path)\n",
    "# img = exposure.equalize_hist(img)\n",
    "# new_shape = (224, 224)\n",
    "# img = transform.resize(img, new_shape, anti_aliasing=True)\n",
    "\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction)\n",
    "print(val_generator.class_indices)\n",
    "\n",
    "# plt.figure(1,1)\n",
    "plt.title(f\"File Name: {img_path}, \\n Prediction: Window is  {'Close' if prediction < CLASS_THRESHOLD else 'Open'}\")\n",
    "plt.imshow(img)\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all images in test folder and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = os.listdir(VAL_DATA_DIR + r\"/close/\")\n",
    "# filelist = os.listdir(TEST_DATA_DIR)\n",
    "jpegfiles = [file for file in filelist if file.endswith('.jpeg')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanted to shrink the images down and display it inside a grid here.. work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## loop through file list and make a grid of images\n",
    "# cols = 3\n",
    "# rows = -(-jpegfiles.count // cols)\n",
    "# fig, axes = plt.subplot(3, rows, figsize(18,6))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Plot each image in the corresponding subplot\n",
    "# for i, image in enumerate(images):\n",
    "#     img = image.load_img(os.path.join(VAL_DATA_DIR  + r\"/close/\",file), target_size=(WIDTH,HEIGHT))\n",
    "#     # img = image.load_img(os.path.join(TEST_DATA_DIR,file), target_size=(WIDTH,HEIGHT))\n",
    "#     img_array = image.img_to_array(img)\n",
    "#     expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "#     preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "#     prediction = model.predict(preprocessed_img)\n",
    "#     print(prediction)\n",
    "#     print(val_generator.class_indices)\n",
    "\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(img)\n",
    "#     ax.title(f\"File Name: {file}, \\n Prediction: Window is  {'Close' if prediction < CLASS_THRESHOLD else 'Open'}\")\n",
    "\n",
    "#     ax.axis('off')  # Turn off axis labels\n",
    "\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in jpegfiles:\n",
    "    img = image.load_img(os.path.join(VAL_DATA_DIR  + r\"/close/\",file), target_size=(WIDTH,HEIGHT))\n",
    "    # img = image.load_img(os.path.join(TEST_DATA_DIR,file), target_size=(WIDTH,HEIGHT))\n",
    "    img_array = image.img_to_array(img)\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "    prediction = model.predict(preprocessed_img)\n",
    "    print(prediction)\n",
    "    print(val_generator.class_indices)\n",
    "\n",
    "    plt.title(f\"File Name: {file}, \\n Prediction: Window is  {'Close' if prediction < CLASS_THRESHOLD else 'Open'}\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
